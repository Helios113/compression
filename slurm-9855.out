/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'base': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=10, no round_timeout
model:
  _target_: torchvision.models.resnet18
  num_classes: ${foo}
strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 1.0e-05
  min_fit_clients: ${num_clients_per_round_fit}
  fraction_evaluate: 1.0e-05
  min_evaluate_clients: ${num_clients_per_round_eval}
  min_available_clients: ${num_clients}
  on_fit_config_fn:
    _target_: server.get_on_fit_config
    config: ${config_fit}
foo: 10
num_rounds: 10
num_clients: 100
batch_size: 20
num_classes: 10
num_clients_per_round_fit: 10
num_clients_per_round_eval: 25
config_fit:
  lr: 0.01
  momentum: 0.9
  local_epochs: 1

[2024-08-14 18:36:15,883][flwr][INFO] - Starting Flower simulation, config: num_rounds=10, no round_timeout
2024-08-14 18:36:18,358	INFO worker.py:1752 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'object_store_memory': 77283108864.0, 'CPU': 56.0, 'node:128.232.115.66': 1.0, 'node:__internal_head__': 1.0, 'memory': 170327254016.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
[2024-08-14 18:36:24,453][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 77283108864.0, 'CPU': 56.0, 'node:128.232.115.66': 1.0, 'node:__internal_head__': 1.0, 'memory': 170327254016.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-08-14 18:36:24,454][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-08-14 18:36:24,455][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 28 actors
[2024-08-14 18:36:24,494][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 28 actors
[92mINFO [0m:      [INIT]
[2024-08-14 18:36:24,497][flwr][INFO] - [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[2024-08-14 18:36:24,498][flwr][INFO] - Requesting initial parameters from one random client
[33m(raylet)[0m [2024-08-14 18:36:28,309 E 3744362 3744392] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-14_18-36-16_019540_3744128 is over 95% full, available space: 13811404800; capacity: 2015100878848. Object creation will fail if spilling is required.
[36m(ClientAppActor pid=3747557)[0m [93mWARNING [0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter "cid: str">}. You can import the `Context` like this: `from flwr.common import Context`
[36m(ClientAppActor pid=3747557)[0m 
[36m(ClientAppActor pid=3747557)[0m             This is a deprecated feature. It will be removed
[36m(ClientAppActor pid=3747557)[0m             entirely in future versions of Flower.
[36m(ClientAppActor pid=3747557)[0m         
[92mINFO [0m:      Received initial parameters from one random client
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[36m(ClientAppActor pid=3747557)[0m tensor(0)
[36m(ClientAppActor pid=3747557)[0m [0]
[2024-08-14 18:36:31,966][flwr][INFO] - Received initial parameters from one random client
[92mINFO [0m:      Evaluating initial global parameters
[2024-08-14 18:36:31,967][flwr][INFO] - Evaluating initial global parameters
[91mERROR [0m:     Error(s) in loading state_dict for ResNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).
[2024-08-14 18:36:32,316][flwr][ERROR] - Error(s) in loading state_dict for ResNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).
[91mERROR [0m:     Traceback (most recent call last):
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/simulation/app.py", line 339, in start_simulation
    hist = run_fl(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 490, in run_fl
    hist, elapsed_time = server.fit(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 95, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/strategy/fedavg.py", line 167, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/nfs-share/pa511/resnet/server.py", line 33, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1671, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ResNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).

[2024-08-14 18:36:32,329][flwr][ERROR] - Traceback (most recent call last):
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/simulation/app.py", line 339, in start_simulation
    hist = run_fl(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 490, in run_fl
    hist, elapsed_time = server.fit(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 95, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/strategy/fedavg.py", line 167, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/nfs-share/pa511/resnet/server.py", line 33, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1671, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ResNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).

[91mERROR [0m:     Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.0}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
[2024-08-14 18:36:32,329][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.0}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/simulation/app.py", line 339, in start_simulation
    hist = run_fl(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 490, in run_fl
    hist, elapsed_time = server.fit(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/server.py", line 95, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/server/strategy/fedavg.py", line 167, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/nfs-share/pa511/resnet/server.py", line 33, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1671, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ResNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 63, in main
    history = fl.simulation.start_simulation(
  File "/nfs-share/pa511/.cache/pypoetry/virtualenvs/advanced-pytorch-4G2CGpb3-py3.8/lib/python3.8/site-packages/flwr/simulation/app.py", line 375, in start_simulation
    raise RuntimeError("Simulation crashed.") from ex
RuntimeError: Simulation crashed.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
srun: error: ngongotaha: task 0: Exited with exit code 1
